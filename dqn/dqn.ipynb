{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Soccer:\n",
    "    '''\n",
    "    Actions [0 : Left, 1 : Up, 2 : Right, 3 : Down, 4 : Stand]\n",
    "    '''\n",
    "    def __init__(self, h=4, w=5, pA=[3, 2], pB=[1, 1], goalPositions=[1, 2], ballOwner=0, drawProbability=0):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        self.goalPositions = np.array(goalPositions)\n",
    "        self.positions = np.array([pA, pB])\n",
    "        self.initPositions = np.array([pA, pB])\n",
    "        self.ballOwner = ballOwner\n",
    "        self.drawProbability = drawProbability\n",
    "        #self.reward = 0\n",
    "        self.rewards = np.array([0, 0])\n",
    "\n",
    "    def reset(self, pA=None, pB=None, ballOwner=None):\n",
    "        if pA is not None:\n",
    "            self.initPositions[0] = pA\n",
    "\n",
    "        if pB is not None:\n",
    "            self.initPositions[1] = pB\n",
    "\n",
    "        if ballOwner is None:\n",
    "            ballOwner = self.choosePlayer()\n",
    "\n",
    "        self.positions = self.initPositions.copy()\n",
    "        self.ballOwner = ballOwner\n",
    "        return self.positions[0] / self.w, self.positions[1] / self.h, self.ballOwner\n",
    "\n",
    "    \n",
    "    def _move(self, actionA, actionB):\n",
    "        reward = 0\n",
    "        if np.random.rand() < self.drawProbability:\n",
    "            return self.positions[0] / self.w, self.positions[1]/ self.h, self.ballOwner, reward, -reward, True\n",
    "        first = self.choosePlayer()\n",
    "        actions = [actionA, actionB]\n",
    "        m1 = self.move(first, actions[first])\n",
    "        # print(m1)\n",
    "        if (m1[-1]):\n",
    "            print(f'returning {m1}')\n",
    "            return m1\n",
    "        return self.move(1 - first, actions[1 - first])\n",
    "\n",
    "    def move(self, player, action):\n",
    "        opponent = 1 - player\n",
    "       \n",
    "        newPosition = self.positions[player] + self.actionToMove(action)\n",
    "\n",
    "        reward = 0\n",
    "        #self.rewards = np.array([0, 0])\n",
    "        # If it's opponent position\n",
    "        if (newPosition == self.positions[opponent]).all():\n",
    "            self.ballOwner = opponent\n",
    "        # If it's the goal\n",
    "        elif self.ballOwner is player and self.isInGoal(*newPosition) >= 0:\n",
    "            # reward = -2*( 1 - self.isInGoal(*newPosition)) + 1\n",
    "            # self.rewards = np.array([reward, -reward ])\n",
    "            reward = -2 * (1 - self.isInGoal(*newPosition)) + 1\n",
    "            return self.positions[0] / self.w, self.positions[1] / self.h, self.ballOwner, reward, -reward, True\n",
    "        # If it's in board\n",
    "        elif self.isInBoard(*newPosition):\n",
    "            self.positions[player] = newPosition\n",
    "        return self.positions[0] / self.w, self.positions[1]/ self.h, self.ballOwner, reward, -reward, False\n",
    "\n",
    "    def actionToMove(self, action):\n",
    "        switcher = {\n",
    "            0: [-1, 0],\n",
    "            1: [0, 1],\n",
    "            2: [1, 0],\n",
    "            3: [0, -1],\n",
    "            4: [0, 0],\n",
    "        }\n",
    "        return switcher.get(action)\n",
    "\n",
    "    def isInGoal(self, x, y):\n",
    "        g1, g2 = self.goalPositions\n",
    "        if (g1 <= y <= g2):\n",
    "            if x == -1:\n",
    "                return 1\n",
    "            elif x == self.w:\n",
    "                return 0\n",
    "        return -1\n",
    "\n",
    "    def isInBoard(self, x, y):\n",
    "        return (0 <= x < self.w and 0 <= y < self.h)\n",
    "\n",
    "    def choosePlayer(self):\n",
    "        return np.random.randint(0, 2)\n",
    "\n",
    "    def draw(self, positions=None, ballOwner=None):\n",
    "        positions = self.positions if positions is None else np.array(positions)\n",
    "        ballOwner = self.ballOwner if ballOwner is None else ballOwner\n",
    "\n",
    "        board = ''\n",
    "        for y in range(self.h)[::-1]:\n",
    "            for x in range(self.w):\n",
    "                if ([x, y] == positions[0]).all():\n",
    "                    board += 'A' if ballOwner == 0 else 'a'\n",
    "                elif ([x, y] == positions[1]).all():\n",
    "                    board += 'B' if ballOwner == 1 else 'b'\n",
    "                else:\n",
    "                    board += '-'\n",
    "            board += '\\n'\n",
    "\n",
    "        print(board)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,seed,input_dimension=6):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dimension, 10)  \n",
    "        self.fc2 = nn.Linear(10, 5)  \n",
    "        self.fc3 = nn.Linear(5, 1) \n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.zeros_(self.fc3.bias)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self,input_ls,label_ls):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "        for i in range(len(input_ls)):\n",
    "            inputs = torch.tensor(input_ls[i],dtype=torch.float).unsqueeze(0)\n",
    "            labels = torch.tensor(label_ls[i],dtype=torch.float).unsqueeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "class Q_network:\n",
    "    def __init__(self,max_cap=1000):\n",
    "        seed = random.randint(0, 1000)\n",
    "        self.target_network = self.build_model(seed)\n",
    "        self.online_network = self.build_model(seed)\n",
    "        self.replay_memory = deque()\n",
    "    def build_model(self,seed):\n",
    "        model = Net(seed=seed)\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from soccer import Soccer\n",
    "# from neural_net import Q_network\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def epsilon_greedy(Q_network,state,epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0,num_actions-1)\n",
    "    else:\n",
    "        Q_values = []\n",
    "        for i in range(num_actions):\n",
    "            state_action = np.concatenate((state,np.array([i])),axis=0)\n",
    "            Q_value = Q_network.online_network.forward(torch.Tensor(state_action))\n",
    "            Q_values.append((Q_value,i))\n",
    "        Q_values.sort(reverse=True)\n",
    "        return Q_values[0][1]\n",
    "\n",
    "def main():\n",
    "    env = Soccer(drawProbability=0.01)\n",
    "    player_A = Q_network()\n",
    "    player_B = Q_network()\n",
    "    num_episodes = 1000\n",
    "    epochs = 100000\n",
    "    frequency = 50\n",
    "    epsilon=0.1\n",
    "    stochastic_param=20\n",
    "    gamma=0.9\n",
    "    fill_memory = 100\n",
    "    wins_A = []\n",
    "    wins_B = []\n",
    "    for episode in range(num_episodes):\n",
    "        current_state_A,current_state_B,BallOwner = env.reset()\n",
    "        for epoch in range(epochs):\n",
    "            # for agent a\n",
    "            action_A = epsilon_greedy(player_A,np.concatenate((current_state_A,current_state_B,np.array([BallOwner])),axis=0),epsilon)\n",
    "            action_B = epsilon_greedy(player_B,np.concatenate((current_state_A,current_state_B,np.array([BallOwner])),axis=0),epsilon)\n",
    "            result = env._move(action_A,action_B)\n",
    "            print(result)\n",
    "            next_state_A,next_state_B,next_BallOwner,reward_A,reward_B,done_env = result\n",
    "        \n",
    "            if(len(list(player_A.replay_memory))>1000):\n",
    "                player_A.replay_memory.popleft()\n",
    "            player_A.replay_memory.append((np.concatenate((current_state_A,current_state_B,np.array([BallOwner])),axis=0),action_A,reward_A,np.concatenate((next_state_A,np.array([next_BallOwner])),axis=0),done_env))\n",
    "            if(len(list(player_B.replay_memory))>1000):\n",
    "                player_B.replay_memory.popleft()\n",
    "            player_B.replay_memory.append((np.concatenate((current_state_A,current_state_B,np.array([BallOwner])),axis=0),action_B,reward_B,np.concatenate((next_state_B,np.array([next_BallOwner])),axis=0),done_env))\n",
    "            current_state_A = next_state_A\n",
    "            current_state_B = next_state_B\n",
    "            if epoch < fill_memory and episode == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sample_for_A = random.sample(list(player_A.replay_memory), stochastic_param)\n",
    "                sample_for_B = random.sample(list(player_B.replay_memory), stochastic_param)\n",
    "                input_ls=[]\n",
    "                label_ls=[]\n",
    "                for sample in sample_for_A:\n",
    "                    state,action,reward,next_state,done = sample\n",
    "                    if done:\n",
    "                        target = reward\n",
    "                    else:\n",
    "                        print('printing concat:')\n",
    "                        print(np.concatenate((next_state,[i]),axis=0))\n",
    "                        target = reward + gamma*max([player_A.target_network.forward(torch.Tensor(np.concatenate((next_state,[i]),axis=0))) for i in range(num_actions)])\n",
    "                    input_ls.append(np.concatenate((state,[action]),axis=0))\n",
    "                    label_ls.append(target)\n",
    "                player_A.online_network.train(input_ls,label_ls)\n",
    "                input_ls=[]\n",
    "                label_ls=[]\n",
    "                for sample in sample_for_B:\n",
    "                    state,action,reward,next_state,done = sample\n",
    "                    if done:\n",
    "                        target = reward\n",
    "                    else:\n",
    "                        target = reward + gamma*max([player_B.target_network.forward(torch.Tensor(np.concatenate((next_state,[i]),axis=0))) for i in range(num_actions)])\n",
    "                    input_ls.append(np.concatenate((state,action),axis=0))\n",
    "                    label_ls.append(target)\n",
    "                # player_B.online_network.train(input_ls,label_ls)\n",
    "            if epoch%frequency==0:\n",
    "                player_A.target_network.load_state_dict(player_A.online_network.state_dict())\n",
    "                player_B.target_network.load_state_dict(player_B.online_network.state_dict())\n",
    "            if done_env:\n",
    "                print(\"Episode number: \",episode)\n",
    "                if reward_A == 1:\n",
    "                    wins_A.append(reward_A)\n",
    "                    wins_B.append(0)\n",
    "                elif reward_B == 1:\n",
    "                    wins_A.append(0)\n",
    "                    wins_B.append(reward_B)\n",
    "                else:\n",
    "                    wins_A.append(0)\n",
    "                    wins_B.append(0)\n",
    "                break \n",
    "    plt.plot(np.cumsum(np.array(wins_A)),label='Player A')\n",
    "    plt.plot(np.cumsum(np.array(wins_B)),label='Player B')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, True)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, True)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, True)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.6, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0.  , 0.25]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.4]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0. , 0.5]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.25, 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.25, 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.25, 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.4, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n",
      "(array([0.6, 0.6]), array([0.  , 0.75]), 0, 0, 0, False)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x4 and 6x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[108], line 61\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmax\u001b[39m([player_A\u001b[38;5;241m.\u001b[39mtarget_network\u001b[38;5;241m.\u001b[39mforward(torch\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39mconcatenate((next_state,[i]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_actions)])\n\u001b[1;32m     62\u001b[0m input_ls\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mconcatenate((state,[action]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     63\u001b[0m label_ls\u001b[38;5;241m.\u001b[39mappend(target)\n",
      "Cell \u001b[0;32mIn[108], line 61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmax\u001b[39m([\u001b[43mplayer_A\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_actions)])\n\u001b[1;32m     62\u001b[0m input_ls\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mconcatenate((state,[action]),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     63\u001b[0m label_ls\u001b[38;5;241m.\u001b[39mappend(target)\n",
      "Cell \u001b[0;32mIn[102], line 23\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x4 and 6x10)"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
