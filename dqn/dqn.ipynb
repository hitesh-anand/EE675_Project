{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,seed,input_dimension=6):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dimension, 10)  \n",
    "        self.fc2 = nn.Linear(10, 5)  \n",
    "        self.fc3 = nn.Linear(5, 1) \n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.zeros_(self.fc3.bias)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def train(self,input_ls,label_ls):\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.01)\n",
    "        for i in range(len(input_ls)):\n",
    "            inputs = torch.tensor(input_ls[i],dtype=torch.float).unsqueeze(0)\n",
    "            labels = torch.tensor(label_ls[i],dtype=torch.float).unsqueeze(0)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "class Q_network:\n",
    "    def __init__(self,max_cap=1000):\n",
    "        seed = random.randint(0, 1000)\n",
    "        self.target_network = self.build_model(seed)\n",
    "        self.online_network = self.build_model(seed)\n",
    "        self.replay_memory = deque()\n",
    "    def build_model(self,seed):\n",
    "        model = Net(seed=seed)\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soccer import Soccer\n",
    "# from neural_net import Q_network\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def epsilon_greedy(Q_network,state,epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0,5)\n",
    "    else:\n",
    "        Q_values = []\n",
    "        for i in range(6):\n",
    "            state_action = np.concatenate((state,i),axis=0)\n",
    "            Q_value = Q_network.online_network.forward(state_action)\n",
    "            Q_values.append((Q_value,i))\n",
    "        Q_values.sort(reverse=True)\n",
    "        return Q_values[0][1]\n",
    "\n",
    "def main():\n",
    "    env = Soccer(drawProbability=0.01)\n",
    "    player_A = Q_network()\n",
    "    player_B = Q_network()\n",
    "    num_episodes = 1000\n",
    "    epochs = 100000\n",
    "    frequency = 50\n",
    "    epsilon=0.1\n",
    "    stochastic_param=20\n",
    "    gamma=0.9\n",
    "    fill_memory = 100\n",
    "    wins_A = []\n",
    "    wins_B = []\n",
    "    for episode in range(num_episodes):\n",
    "        current_state_A,current_state_B,BallOwner = env.restart()\n",
    "        for epoch in range(epochs):\n",
    "            # for agent a\n",
    "            action_A = epsilon_greedy(player_A,np.concatenate((current_state_A,BallOwner),axis=0),epsilon)\n",
    "            action_B = epsilon_greedy(player_B,np.concatenate((current_state_B,BallOwner),axis=0),epsilon)\n",
    "            next_state_A,next_state_B,next_BallOwner,reward_A,reward_B,done_env = env.move(action_A,action_B)\n",
    "            if(len(list(player_A.replay_memory))>1000):\n",
    "                player_A.replay_memory.popleft()\n",
    "            player_A.replay_memory.append((np.concatenate((current_state_A,current_state_B,BallOwner),axis=0),action_A,reward_A,np.concatenate((next_state_A,next_BallOwner),axis=0),done_env))\n",
    "            if(len(list(player_B.replay_memory))>1000):\n",
    "                player_B.replay_memory.popleft()\n",
    "            player_B.replay_memory.append((np.concatenate((current_state_A,current_state_B,BallOwner),axis=0),action_B,reward_B,np.concatenate((next_state_B,next_BallOwner),axis=0),done_env))\n",
    "            current_state_A = next_state_A\n",
    "            current_state_B = next_state_B\n",
    "            if epoch < fill_memory and episode == 0:\n",
    "                continue\n",
    "            else:\n",
    "                sample_for_A = random.sample(list(player_A.replay_memory), stochastic_param)\n",
    "                sample_for_B = random.sample(list(player_B.replay_memory), stochastic_param)\n",
    "                input_ls=[]\n",
    "                label_ls=[]\n",
    "                for sample in sample_for_A:\n",
    "                    state,action,reward,next_state,done = sample\n",
    "                    if done:\n",
    "                        target = reward\n",
    "                    else:\n",
    "                        target = reward + gamma*max([player_A.target_network.forward(np.concatenate((next_state,i),axis=0)) for i in range(6)])\n",
    "                    input_ls.append(np.concatenate((state,action),axis=0))\n",
    "                    label_ls.append(target)\n",
    "                player_A.online_network.train(input_ls,label_ls)\n",
    "                input_ls=[]\n",
    "                label_ls=[]\n",
    "                for sample in sample_for_B:\n",
    "                    state,action,reward,next_state,done = sample\n",
    "                    if done:\n",
    "                        target = reward\n",
    "                    else:\n",
    "                        target = reward + gamma*max([player_B.target_network.forward(np.concatenate((next_state,i),axis=0)) for i in range(6)])\n",
    "                    input_ls.append(np.concatenate((state,action),axis=0))\n",
    "                    label_ls.append(target)\n",
    "                # player_B.online_network.train(input_ls,label_ls)\n",
    "            if epoch%frequency==0:\n",
    "                player_A.target_network.load_state_dict(player_A.online_network.state_dict())\n",
    "                player_B.target_network.load_state_dict(player_B.online_network.state_dict())\n",
    "            if done_env:\n",
    "                if reward_A == 1:\n",
    "                    wins_A.append(reward_A)\n",
    "                    wins_B.append(0)\n",
    "                elif reward_B == 1:\n",
    "                    wins_A.append(0)\n",
    "                    wins_B.append(reward_B)\n",
    "                else:\n",
    "                    wins_A.append(0)\n",
    "                    wins_B.append(0)\n",
    "                break \n",
    "    plt.plot(np.cumsum(np.array(wins_A)),label='Player A')\n",
    "    plt.plot(np.cumsum(np.array(wins_B)),label='Player B')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Soccer' object has no attribute 'reset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 32\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m wins_B \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 32\u001b[0m     current_state_A,current_state_B,BallOwner \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m()\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# for agent a\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         action_A \u001b[38;5;241m=\u001b[39m epsilon_greedy(player_A,np\u001b[38;5;241m.\u001b[39mconcatenate((current_state_A,BallOwner),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),epsilon)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Soccer' object has no attribute 'reset'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
